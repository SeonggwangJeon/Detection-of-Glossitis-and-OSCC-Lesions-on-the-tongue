{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1536cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('/home/bis/230711_JSG/230602_JSG_Image/')\n",
    "from Mymodule.ModelHandler import *\n",
    "from Mymodule.Utils import *\n",
    "from Mymodule.GradCam import *\n",
    "from Mymodule.BatchHandler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2575b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2')\n",
    "model_name = 'vgg16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ee0261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explort_imgs(images, parent_dir,candidates):\n",
    "    for i,image in enumerate(images):\n",
    "        file_path = os.path.join(parent_dir, f'{candidates[i]}')\n",
    "        print(file_path)\n",
    "        cv2.imwrite(file_path, image)\n",
    "    print('exported..done')\n",
    "    \n",
    "def calculate_zero_ratios(img):\n",
    "    w, h = img.shape\n",
    "    zeros = 0\n",
    "    for i in range(w):\n",
    "        for j in range(w):\n",
    "            if img[i, j] <= 0:\n",
    "                zeros += 1\n",
    "    return zeros / (w*h)\n",
    "\n",
    "class LayerActivation():\n",
    "    features = None    \n",
    "    def __init__(self, model, layer_num):\n",
    "        self.hook = model.base.features[layer_num].register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features = output.cpu().data.numpy()\n",
    "    def remove(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "        \n",
    "activation = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1772ea30",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mimage_path\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_path' is not defined"
     ]
    }
   ],
   "source": [
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb1ef85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 1)\n"
     ]
    }
   ],
   "source": [
    "#candidates = ['10749325(2)_56_0', '10859932(2)_78_0', '10749325(3)_56_1']\n",
    "candidates = ['14934497_23_1', '14985233_40_1', '14943382_51_1']\n",
    "candidates2 = ['14934497 23 2', '14985233 40 2', '14943382 51 2']\n",
    "\n",
    "root = './Data/cropped_images/'\n",
    "\n",
    "images = {}\n",
    "image_names = {}\n",
    "crops = [1,3,4,6]\n",
    "for crop in crops:\n",
    "    images[crop] = []\n",
    "    image_names[crop] = []\n",
    "    for i, candidate in enumerate(candidates):\n",
    "        if crop != 1 : image_root = os.path.join(root,'crop'+f'_{crop}', 'val', candidate)\n",
    "        if crop == 1 : image_root = os.path.join('./Images/Validation/',candidates2[i])\n",
    "        surfixs = []\n",
    "        for i in range(crop):\n",
    "            surfixs.append(f'_part_{i+1}.png')\n",
    "        for surfix in surfixs:\n",
    "            if crop != 1: image_path = image_root + surfix\n",
    "            if crop == 1: image_path = image_root + '.png'\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.resize(image,(140,140))\n",
    "            images[crop].append(image)\n",
    "            image_names[crop].append(image_path.split('/')[-1])\n",
    "    images[crop] = np.array(images[crop])\n",
    "    test_y = np.zeros([9,1])\n",
    "\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c66d05a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Mymodule.GradCam.ActivationsAndGradients object at 0x7fedb30ce560>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./GradCam_cropped_images/1/14934497 23 2.png\n",
      "./GradCam_cropped_images/1/14985233 40 2.png\n",
      "./GradCam_cropped_images/1/14943382 51 2.png\n",
      "exported..done\n",
      "<Mymodule.GradCam.ActivationsAndGradients object at 0x7fedb27bb8e0>\n",
      "./GradCam_cropped_images/3/14934497_23_1_part_1.png\n",
      "./GradCam_cropped_images/3/14934497_23_1_part_2.png\n",
      "./GradCam_cropped_images/3/14934497_23_1_part_3.png\n",
      "./GradCam_cropped_images/3/14985233_40_1_part_1.png\n",
      "./GradCam_cropped_images/3/14985233_40_1_part_2.png\n",
      "./GradCam_cropped_images/3/14985233_40_1_part_3.png\n",
      "./GradCam_cropped_images/3/14943382_51_1_part_1.png\n",
      "./GradCam_cropped_images/3/14943382_51_1_part_2.png\n",
      "./GradCam_cropped_images/3/14943382_51_1_part_3.png\n",
      "exported..done\n",
      "<Mymodule.GradCam.ActivationsAndGradients object at 0x7fed9ad94df0>\n",
      "./GradCam_cropped_images/4/14934497_23_1_part_1.png\n",
      "./GradCam_cropped_images/4/14934497_23_1_part_2.png\n",
      "./GradCam_cropped_images/4/14934497_23_1_part_3.png\n",
      "./GradCam_cropped_images/4/14934497_23_1_part_4.png\n",
      "./GradCam_cropped_images/4/14985233_40_1_part_1.png\n",
      "./GradCam_cropped_images/4/14985233_40_1_part_2.png\n",
      "./GradCam_cropped_images/4/14985233_40_1_part_3.png\n",
      "./GradCam_cropped_images/4/14985233_40_1_part_4.png\n",
      "./GradCam_cropped_images/4/14943382_51_1_part_1.png\n",
      "./GradCam_cropped_images/4/14943382_51_1_part_2.png\n",
      "./GradCam_cropped_images/4/14943382_51_1_part_3.png\n",
      "./GradCam_cropped_images/4/14943382_51_1_part_4.png\n",
      "exported..done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/230711_JSG/230602_JSG_Image/Mymodule/GradCam.py:119: RuntimeWarning: invalid value encountered in divide\n",
      "  img = img / np.max(img)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Mymodule.GradCam.ActivationsAndGradients object at 0x7fed9ae0f760>\n",
      "./GradCam_cropped_images/6/14934497_23_1_part_1.png\n",
      "./GradCam_cropped_images/6/14934497_23_1_part_2.png\n",
      "./GradCam_cropped_images/6/14934497_23_1_part_3.png\n",
      "./GradCam_cropped_images/6/14934497_23_1_part_4.png\n",
      "./GradCam_cropped_images/6/14934497_23_1_part_5.png\n",
      "./GradCam_cropped_images/6/14934497_23_1_part_6.png\n",
      "./GradCam_cropped_images/6/14985233_40_1_part_1.png\n",
      "./GradCam_cropped_images/6/14985233_40_1_part_2.png\n",
      "./GradCam_cropped_images/6/14985233_40_1_part_3.png\n",
      "./GradCam_cropped_images/6/14985233_40_1_part_4.png\n",
      "./GradCam_cropped_images/6/14985233_40_1_part_5.png\n",
      "./GradCam_cropped_images/6/14985233_40_1_part_6.png\n",
      "./GradCam_cropped_images/6/14943382_51_1_part_1.png\n",
      "./GradCam_cropped_images/6/14943382_51_1_part_2.png\n",
      "./GradCam_cropped_images/6/14943382_51_1_part_3.png\n",
      "./GradCam_cropped_images/6/14943382_51_1_part_4.png\n",
      "./GradCam_cropped_images/6/14943382_51_1_part_5.png\n",
      "./GradCam_cropped_images/6/14943382_51_1_part_6.png\n",
      "exported..done\n"
     ]
    }
   ],
   "source": [
    "for crop in crops:\n",
    "    test_loader = GetLoader([], images[crop], np.zeros([crop*3,1]), batch=crop*3, test=True)      \n",
    "    for train_strategy in ['finetuning']:\n",
    "        for i,(data, label) in enumerate(test_loader):\n",
    "            data = data\n",
    "            label = label\n",
    "            if crop != 1 : model_save_path = f'./Model/voting{crop}_{model_name}_{train_strategy}.pt'\n",
    "            if crop == 1 : model_save_path = f'./Model/{model_name}_{train_strategy}.pt'\n",
    "\n",
    "            model = get_model(model_name, device, pretrained=True)    \n",
    "            model.load_state_dict(torch.load(model_save_path))\n",
    "            model.eval()\n",
    "\n",
    "            last_layer = model.base.features[-2]\n",
    "            cam = GradCAM(model=model, target_layer=last_layer, device=device)\n",
    "            grayscale_cam = cam(input_tensor=data, target_category=0)\n",
    "            \n",
    "    visuals = get_visuals(images[crop]/255, grayscale_cam)\n",
    "    explort_imgs(visuals, f'./GradCam_cropped_images/{crop}/', image_names[crop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d730df40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41118cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "9f16a74f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[326], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m100\u001b[39m:\n\u001b[1;32m     15\u001b[0m         ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m20\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, xticks\u001b[38;5;241m=\u001b[39m[], yticks\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m---> 16\u001b[0m         ax\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mmaps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[i])\n\u001b[1;32m     17\u001b[0m     mean_zero_ratio \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m calculate_zero_ratios(maps[\u001b[38;5;241m0\u001b[39m][i])\n\u001b[1;32m     19\u001b[0m mean_zero_ratio \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAE8AAABPCAYAAACqNJiGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABS0lEQVR4nO3cPYrDMBQA4WezrZTeoPsfTOADSL2VyluFROwQst6dD9zJIA3+6d4yxhihH1k/vYErMx5gPMB4gPEA4wHGA4wHfM0sOo4j9n2PlFIsy/LuPX3UGCN677FtW6zri2drTKi1joj4V1et9WWXqScvpRQREbXWyDnP3HJZrbUopXyf+ZmpeOermnP+8/FOM58nfxiA8QDjAcYDjAcYDzAeYDzAeIDxAOMBxgOMBxgPMB5gPMB4gPEA4wHGA4wHGA8wHmA8wHiA8QDjAcYDjAcYDzAeYDzAeIDxAOMBxgOMBxgPMB5gPMB4gPEA4wHGA4wHGA8wHmA8wHiA8QDjAcYDjAcYDzAeMDVX5Rwr2lp762Z+g/OMM6NUp+L13iMiopQCtnUtvfe43W5P1ywz02odxPXYVDw95g8DMB5gPMB4gPEA4wHGA+72AsPsv7qdbAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1300x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = get_model(model_name, device, pretrained=True)    \n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "conv_out = LayerActivation(model, 42)\n",
    "conv_out.remove()\n",
    "maps = conv_out.features\n",
    "\n",
    "fig = plt.figure(figsize=(13,4))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=0.8, hspace=0.1, wspace=0.1)\n",
    "\n",
    "mean_zero_ratio = 0.0\n",
    "\n",
    "for i in range(512):\n",
    "    if i < 100:\n",
    "        ax = fig.add_subplot(5, 20, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(maps[0][i])\n",
    "    mean_zero_ratio += calculate_zero_ratios(maps[0][i])\n",
    "\n",
    "mean_zero_ratio /= 512\n",
    "print('%.4f' % mean_zero_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd40db41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JSG_image",
   "language": "python",
   "name": "jsg_image"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
